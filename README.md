# Introduction 
Used to ingest data, apply expected schema to it and perform some basic validations. The output is intended to be consumed my the helper-process pipeline.

# Getting Started
Complete git and aws install, and configure.

### Clone the Repo

From the command line pull down the latest code:

### Install the development dependency for spark_process_common

Inside the working directory for the current project, run the following command to create a local development install of the process_common project.

Running Tests
---

In order to run the tests, you will need to have pyspark available locally.
