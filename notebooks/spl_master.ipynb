{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to read files directly from S3.  It only needs to be executed once per kernel session.  \n",
    "# The keys can be found in the AWS Resources wiki under the Enterprise Sales tab in teams.\n",
    "\n",
    "#spark.conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "#spark.conf.set(\"fs.s3a.awsAccessKeyId\", \"AKIAIKAN6M4XJUFNKY7Q\")\n",
    "#spark.conf.set(\"fs.s3a.awsSecretAccessKey\", \"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\")\n",
    "\n",
    "spark.conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "spark.conf.set(\"fs.s3.awsAccessKeyId\", \"AKIAIKAN6M4XJUFNKY7Q\")\n",
    "spark.conf.set(\"fs.s3.awsSecretAccessKey\", \"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\")\n",
    "\n",
    "#sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAIKAN6M4XJUFNKY7Q\")\n",
    "#sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## There are run-once while testing\n",
    "ddl = \"CREATE DATABASE IF NOT EXISTS stage\"\n",
    "spark.sql(ddl)\n",
    "\n",
    "ddl = \"CREATE DATABASE IF NOT EXISTS spl_master\"\n",
    "spark.sql(ddl)\n",
    "\n",
    "ddl = \"DROP TABLE IF EXISTS stage.spl_master_mstr_summary_product_level\"\n",
    "spark.sql(ddl)\n",
    "\n",
    "ddl = \"CREATE TABLE if not exists stage.spl_master_mstr_summary_product_level( \\\n",
    "       spl_id string, \\\n",
    "       spl_desc string, \\\n",
    "       business_unit_id smallint, \\\n",
    "       new_segment_id string, \\\n",
    "       iptmeta_corrupt_record string, \\\n",
    "       iptmeta_extract_dttm timestamp) \\\n",
    "       PARTITIONED BY(iptmeta_record_origin string) \\\n",
    "       STORED AS ORC\"\n",
    "spark.sql(ddl)\n",
    "\n",
    "ddl = \"DROP TABLE IF EXISTS spl_master.mstr_summary_product_level\"\n",
    "spark.sql(ddl)\n",
    "\n",
    "ddl = \"CREATE TABLE if not exists spl_master.mstr_summary_product_level( \\\n",
    "       spl_id string, \\\n",
    "       spl_desc string, \\\n",
    "       business_unit_id smallint, \\\n",
    "       new_segment_id string, \\\n",
    "       iptmeta_corrupt_record string, \\\n",
    "       iptmeta_extract_dttm timestamp) \\\n",
    "       STORED AS ORC\"\n",
    "spark.sql(ddl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_process_common.ingest import IngestExcelFile\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# NOTE: \"hive.exec.dynamic.partition.mode\": \"nonstrict\" was added to spark_config. This allowed the code to run.\n",
    "# It was raised in the output on ingest.execute() as: \"hive.exec.dynamic.partition.mode=nonstrict\"\n",
    "\n",
    "spark_config = {\n",
    "    \"spark.sql.hive.convertMetastoreOrc\": \"true\",\n",
    "    \"spark.sql.files.ignoreMissingFiles\": \"true\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \"spark.sql.hive.verifyPartitionPath\": \"false\",\n",
    "    \"spark.sql.orc.filterPushdown\": \"true\",\n",
    "    \"hive.exec.dynamic.partition.mode\": \"nonstrict\",\n",
    "    \"hive.exec.dynamic.partition\": \"true\"\n",
    "}\n",
    "\n",
    "json_config = {\n",
    "    \n",
    "    \"config_bucket\": \"/vagrant/aws/job-config/spl_master/\",\n",
    "    \"config_path\": \"summary_product_level.json\"\n",
    "}\n",
    "    \n",
    "# Ingest \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ingest = IngestExcelFile(spark_config=spark_config, json_config=json_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ingest.get_config()\n",
    "#make modifications here\n",
    "df = ingest.extract(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config_bucket': '/vagrant/aws/job-config/spl_master/', 'config_path': 'summary_product_level.json'}\n"
     ]
    }
   ],
   "source": [
    "   #\"io\": \"/vagrant/notebooks/spark-warehouse/Com Print Mapping Data.xlsx\",\n",
    "print(json_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the live outputs for ingest processing of Cost Master File\n",
    "df = spark.sql(\"SELECT * from stage.spl_master_mstr_summary_product_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------------+--------------+----------------------+-----------------------+---------------------+\n",
      "|spl_id|spl_desc            |business_unit_id|new_segment_id|iptmeta_corrupt_record|iptmeta_extract_dttm   |iptmeta_record_origin|\n",
      "+------+--------------------+----------------+--------------+----------------------+-----------------------+---------------------+\n",
      "|EM400 |TOBACCO STD         |20              |FBEV          |null                  |2018-09-06 17:57:06.881|SOURCE               |\n",
      "|EM405 |CONSUMER OFFICE PROD|30              |9999          |null                  |2018-09-06 17:57:06.881|SOURCE               |\n",
      "|EM410 |SPECIALTY CHEMICALS |50              |SPCD          |null                  |2018-09-06 17:57:06.881|SOURCE               |\n",
      "|EM415 |COMM DEV & LAND MGMT|40              |CDLM          |null                  |2018-09-06 17:57:06.881|SOURCE               |\n",
      "|EM420 |SPECIALTY PAPERS    |60              |9999          |null                  |2018-09-06 17:57:06.881|SOURCE               |\n",
      "+------+--------------------+----------------+--------------+----------------------+-----------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------------+--------------+----------------------+--------------------+---------------------+\n",
      "|spl_id|spl_desc|business_unit_id|new_segment_id|iptmeta_corrupt_record|iptmeta_extract_dttm|iptmeta_record_origin|\n",
      "+------+--------+----------------+--------------+----------------------+--------------------+---------------------+\n",
      "+------+--------+----------------+--------------+----------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find any rows that failed validation\n",
    "df.where(\"iptmeta_corrupt_record is not null\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------------+--------------+----------------------+--------------------+---------------------+\n",
      "|spl_id|            spl_desc|business_unit_id|new_segment_id|iptmeta_corrupt_record|iptmeta_extract_dttm|iptmeta_record_origin|\n",
      "+------+--------------------+----------------+--------------+----------------------+--------------------+---------------------+\n",
      "| EM400|         TOBACCO STD|              20|          FBEV|                  null|2018-09-06 17:57:...|               SOURCE|\n",
      "| EM405|CONSUMER OFFICE PROD|              30|          9999|                  null|2018-09-06 17:57:...|               SOURCE|\n",
      "| EM410| SPECIALTY CHEMICALS|              50|          SPCD|                  null|2018-09-06 17:57:...|               SOURCE|\n",
      "| EM415|COMM DEV & LAND MGMT|              40|          CDLM|                  null|2018-09-06 17:57:...|               SOURCE|\n",
      "| EM420|    SPECIALTY PAPERS|              60|          9999|                  null|2018-09-06 17:57:...|               SOURCE|\n",
      "+------+--------------------+----------------+--------------+----------------------+--------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find any rows that did not fail validation\n",
    "df.where(\"iptmeta_corrupt_record is null\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.3.0 (venv3)",
   "language": "python",
   "name": "pyspark_venv36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
